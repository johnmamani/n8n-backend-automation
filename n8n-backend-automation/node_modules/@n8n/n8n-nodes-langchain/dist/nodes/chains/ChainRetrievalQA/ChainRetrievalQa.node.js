"use strict";
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var ChainRetrievalQa_node_exports = {};
__export(ChainRetrievalQa_node_exports, {
  ChainRetrievalQa: () => ChainRetrievalQa
});
module.exports = __toCommonJS(ChainRetrievalQa_node_exports);
var import_prompts = require("@langchain/core/prompts");
var import_combine_documents = require("langchain/chains/combine_documents");
var import_retrieval = require("langchain/chains/retrieval");
var import_n8n_workflow = require("n8n-workflow");
var import_descriptions = require("../../../utils/descriptions");
var import_helpers = require("../../../utils/helpers");
var import_sharedFields = require("../../../utils/sharedFields");
var import_tracing = require("../../../utils/tracing");
const SYSTEM_PROMPT_TEMPLATE = `You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
----------------
Context: {context}`;
const LEGACY_INPUT_TEMPLATE_KEY = "question";
const INPUT_TEMPLATE_KEY = "input";
const systemPromptOption = {
  displayName: "System Prompt Template",
  name: "systemPromptTemplate",
  type: "string",
  default: SYSTEM_PROMPT_TEMPLATE,
  typeOptions: {
    rows: 6
  }
};
class ChainRetrievalQa {
  constructor() {
    this.description = {
      displayName: "Question and Answer Chain",
      name: "chainRetrievalQa",
      icon: "fa:link",
      iconColor: "black",
      group: ["transform"],
      version: [1, 1.1, 1.2, 1.3, 1.4, 1.5],
      description: "Answer questions about retrieved documents",
      defaults: {
        name: "Question and Answer Chain",
        color: "#909298"
      },
      codex: {
        alias: ["LangChain"],
        categories: ["AI"],
        subcategories: {
          AI: ["Chains", "Root Nodes"]
        },
        resources: {
          primaryDocumentation: [
            {
              url: "https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/"
            }
          ]
        }
      },
      // eslint-disable-next-line n8n-nodes-base/node-class-description-inputs-wrong-regular-node
      inputs: [
        import_n8n_workflow.NodeConnectionTypes.Main,
        {
          displayName: "Model",
          maxConnections: 1,
          type: import_n8n_workflow.NodeConnectionTypes.AiLanguageModel,
          required: true
        },
        {
          displayName: "Retriever",
          maxConnections: 1,
          type: import_n8n_workflow.NodeConnectionTypes.AiRetriever,
          required: true
        }
      ],
      outputs: [import_n8n_workflow.NodeConnectionTypes.Main],
      credentials: [],
      properties: [
        (0, import_sharedFields.getTemplateNoticeField)(1960),
        {
          displayName: "Query",
          name: "query",
          type: "string",
          required: true,
          default: "={{ $json.input }}",
          displayOptions: {
            show: {
              "@version": [1]
            }
          }
        },
        {
          displayName: "Query",
          name: "query",
          type: "string",
          required: true,
          default: "={{ $json.chat_input }}",
          displayOptions: {
            show: {
              "@version": [1.1]
            }
          }
        },
        {
          displayName: "Query",
          name: "query",
          type: "string",
          required: true,
          default: "={{ $json.chatInput }}",
          displayOptions: {
            show: {
              "@version": [1.2]
            }
          }
        },
        {
          ...import_descriptions.promptTypeOptions,
          displayOptions: {
            hide: {
              "@version": [{ _cnd: { lte: 1.2 } }]
            }
          }
        },
        {
          ...import_descriptions.textFromPreviousNode,
          displayOptions: { show: { promptType: ["auto"], "@version": [{ _cnd: { gte: 1.4 } }] } }
        },
        {
          displayName: "Prompt (User Message)",
          name: "text",
          type: "string",
          required: true,
          default: "",
          placeholder: "e.g. Hello, how can you help me?",
          typeOptions: {
            rows: 2
          },
          displayOptions: {
            show: {
              promptType: ["define"]
            }
          }
        },
        {
          displayName: "Options",
          name: "options",
          type: "collection",
          default: {},
          placeholder: "Add Option",
          options: [
            {
              ...systemPromptOption,
              description: `Template string used for the system prompt. This should include the variable \`{context}\` for the provided context. For text completion models, you should also include the variable \`{${LEGACY_INPUT_TEMPLATE_KEY}}\` for the user\u2019s query.`,
              displayOptions: {
                show: {
                  "@version": [{ _cnd: { lt: 1.5 } }]
                }
              }
            },
            {
              ...systemPromptOption,
              description: `Template string used for the system prompt. This should include the variable \`{context}\` for the provided context. For text completion models, you should also include the variable \`{${INPUT_TEMPLATE_KEY}}\` for the user\u2019s query.`,
              displayOptions: {
                show: {
                  "@version": [{ _cnd: { gte: 1.5 } }]
                }
              }
            }
          ]
        }
      ]
    };
  }
  async execute() {
    this.logger.debug("Executing Retrieval QA Chain");
    const items = this.getInputData();
    const returnData = [];
    for (let itemIndex = 0; itemIndex < items.length; itemIndex++) {
      try {
        const model = await this.getInputConnectionData(
          import_n8n_workflow.NodeConnectionTypes.AiLanguageModel,
          0
        );
        const retriever = await this.getInputConnectionData(
          import_n8n_workflow.NodeConnectionTypes.AiRetriever,
          0
        );
        let query;
        if (this.getNode().typeVersion <= 1.2) {
          query = this.getNodeParameter("query", itemIndex);
        } else {
          query = (0, import_helpers.getPromptInputByType)({
            ctx: this,
            i: itemIndex,
            inputKey: "text",
            promptTypeKey: "promptType"
          });
        }
        if (query === void 0) {
          throw new import_n8n_workflow.NodeOperationError(this.getNode(), "The \u2018query\u2018 parameter is empty.");
        }
        const options = this.getNodeParameter("options", itemIndex, {});
        let templateText = options.systemPromptTemplate ?? SYSTEM_PROMPT_TEMPLATE;
        if (this.getNode().typeVersion < 1.5) {
          templateText = templateText.replace(
            `{${LEGACY_INPUT_TEMPLATE_KEY}}`,
            `{${INPUT_TEMPLATE_KEY}}`
          );
        }
        let promptTemplate;
        if ((0, import_helpers.isChatInstance)(model)) {
          const messages = [
            import_prompts.SystemMessagePromptTemplate.fromTemplate(templateText),
            import_prompts.HumanMessagePromptTemplate.fromTemplate("{input}")
          ];
          promptTemplate = import_prompts.ChatPromptTemplate.fromMessages(messages);
        } else {
          const questionSuffix = options.systemPromptTemplate === void 0 ? "\n\nQuestion: {input}\nAnswer:" : "";
          promptTemplate = new import_prompts.PromptTemplate({
            template: templateText + questionSuffix,
            inputVariables: ["context", "input"]
          });
        }
        const combineDocsChain = await (0, import_combine_documents.createStuffDocumentsChain)({
          llm: model,
          prompt: promptTemplate
        });
        const retrievalChain = await (0, import_retrieval.createRetrievalChain)({
          combineDocsChain,
          retriever
        });
        const tracingConfig = (0, import_tracing.getTracingConfig)(this);
        const response = await retrievalChain.withConfig(tracingConfig).invoke({ input: query }, { signal: this.getExecutionCancelSignal() });
        const answer = response.answer;
        if (this.getNode().typeVersion >= 1.5) {
          returnData.push({ json: { response: answer } });
        } else {
          returnData.push({ json: { response: { text: answer } } });
        }
      } catch (error) {
        if (this.continueOnFail()) {
          const metadata = (0, import_n8n_workflow.parseErrorMetadata)(error);
          returnData.push({
            json: { error: error.message },
            pairedItem: { item: itemIndex },
            metadata
          });
          continue;
        }
        throw error;
      }
    }
    return [returnData];
  }
}
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  ChainRetrievalQa
});
//# sourceMappingURL=ChainRetrievalQa.node.js.map